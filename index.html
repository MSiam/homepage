<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Metadata of the Webpage -->
    <!-- Character-set Metadata -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <!-- Viewport Metadata -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Description Metadata-->
    <meta name="description" content="Academic Website" />
    <!-- Author Metadata -->
    <meta name="author" content="Mennatullah Siam" />
    <!-- Keyword Metadata -->
    <meta
      name="keywords"
      content="Mennatullah Siam"
    />
    <!-- Webpage Logo -->
    <link rel="shortcut icon" href="./assets/img/favicon.ico" />
    <link rel="stylesheet" href="./assets/css/academicons.min.css"/>

    <!-- Webpage Title -->
    <title>Mennatullah Siam</title>

    <!-- Import CSS: Main Stylesheet -->
    <link rel="stylesheet" href="./assets/css/main.css" />
  </head>

  <body>
    
    <!-- About Section -->
    <section id="about">
      <!-- User Introduction-->
      <div class="user-details">
        <h2 style="text-align:left;">Mennatullah Siam, PhD</h2>

        <div class="images-right">
        <picture style="text-align:right;">
          <img
            src="./assets/img/jpg/photo.jpg"
            alt="Professional Me"
            width="40%"
            style="border-radius: 50%"
          />
        </picture>
        </div>
        <div class="contents">
            <h2></h2>
            <h3 style="text-align:left;">Assistant Professor in Engineering and Applied Science, Ontario Tech University</h3>
	    <h3 style="text-align:left;">Affiliate Assistant Professor in CS, University of British Columbia</h3>
            <p style="text-align:left;">Office: SIRC-3388, <a href="https://calendar.google.com/calendar/u/0?cid=bWVubmEuc2V5YW1AZ21haWwuY29t">Calendar</a>, <a href="https://scholar.google.com/citations?user=AVPds3kAAAAJ&hl=en&oi=ao">Scholar</a>, <a href="https://www.linkedin.com/in/mennatullah-siam-6546508a/">Linkedin</a>, <a href="https://github.com/MSiam">Github</a>, Email: first.lastname@ubc.ca, <a href="https://msiam.github.io/homepage/docs/CV-19.pdf">Academic CV</a></p>
        </div>    
        <p align="justify">
	I am an assistant professor in Ontario Tech University leading the Image and Video Understanding (IVU) lab and an affiliate professor in UBC. My research interests include pixel-level scene and video understanding, data efficient learning, interpretability and responsible AI. Previously I was a Postdoctoral researcher working with Professor Richard Wildes in York University. I was also a vector affiliate. I obtained my PhD in 2021 under Professor Martin Jagersand supervision working in vision for robotics. My thesis was focused on learning video object segmentation from limited labelled data, where I was working on the intersection between video object segmentation and fewshot object segmentation. I was a member in a team of 4 in the KUKA Innovation Challenge 2018, where our team was one of the top 5 finalists. Previously I finished my MSc in NU and BSc in Ainshams University, Egypt.
<br>
<b>Research Interests:</b> Computer Vision, Deep Learning, Fewshot Learning, Video Object Segmentation, Video Understanding, Interpretability, Responsible AI.
<br>
<b>Research Statement:</b> <a href="https://drive.google.com/file/d/1afwZ7MMmcb6jB1hK9us45N5ZB1SQsy1f/view?usp=sharing">This statement</a> only provides a rough description of my research program without delving into details.
<br>
I am heavily promoting "No to Killing children and civilians", "No to Genocide!", "No to undermining Human Rights for some money that will be gone anyway within a limited lifetime". This is an integral component of every course I teach in the first lecture just to regain humanity back again in a fast paced AI research.
If you have the same mindset happy to connect as needed.
	</p>
      </div>
	    
    <section id="talks">
        <div>
            <h1> Talks and Recognitions</h1>
            <ul>
	      <li> "Learning Scene and Video Understanding with Limited Labelled Data", <a href="https://slideslive.com/38995267/learning-scene-and-video-understanding-with-limited-labelled-data">Black in AI Keynote</a>, NeurIPS 2022.
	      <li> "From Scene to Video Understanding, what to Consider?", University of British Columbia, 2022.
	      <li> "On the Intersection of Few-shot and Video Object Segmentation.", Doctoral Consortium, CVPR 2021.
	      <li> "Few-shot Learning Tutorial", Samsung AI, 2022.
	      <li> "Segmentation with Transformers Tutorial", <a href="https://drive.google.com/file/d/1lPJekV7rUFOsr9aoFM4xGdB6iuJEE76I/view?usp=sharing">Ro'ya Workshop</a>, Deep Learning Indaba, Accra, Ghana, 2024.
	      <li> <a href="https://www.youtube.com/watch?v=aLcw73dt_Oo">KUKA Innovation Finalist Award</a>, 2018.
	      <li> <a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php">Outstanding Reviewing</a> in ICCV 2023.
	      <li> PhD/Postdoc scholarships: VISTA Fellowship, AITF, Verna Tate Graduate Scholarship, Alberta Graduate Excellence Scholarship.
	   </ul>
	</div>
	    
    <section id="news">
        <div>
            <h1> News</h1>
            <ul>
	      <li> April 2025: Our work on building a vision centric remote sensing benchmark that was led by AMMI/AIMS MSc student jointly supervisde by me and Prof. Naoto has been accepted in Eval-FoMo CVPR 25 workshop. Congrats to the team and to him for finishing his MSc.  
	      <li> February 2025: My work got accepted in IJCV 2025 that is focused on few-shot video object segmentation, I acknowledge the guidance of my postdoc supervisors although they were not able to continue the work till its final acceptance form.
	      <li> January 2025: I am an organizer in <a href="https://sites.google.com/view/pixfoundation"> PixFoundation: 1st Workshop on Pixel-level Vision Foundation Models in CVPR 2025</a>, You can Follow us on <a href="https://x.com/PixFoundationCV"> Twitter </a> for updates.
	      <li> November 2024: Our work on TAM-VT video segmentation and tracking is accepted in WACV 2025, our work with RIKEN institute was also accepted in IEEE Geoscience and Remote Sensing Letters.
	      <li> October 2024: Our work is accepted in Neuro AI workshop and WiML part of NeurIPS 2024.
	      <li> September 2024: Our work is accepted in TPAMI, which was an extension of our CVPR 2022 paper.</li>
	      <li> August 2024: Our work on the current state of Computer Vision research in Africa is accepted in JAIR special issue on Fairness and Bias in AI.
	      <li> June 2024: I am WACV 2025 Area Chair.
	      <li> May 2024: I acquired the NSERC Alliance International grant, thanks to NSERC.
	      <li> April 2024: Happy to announce that I acquired the Discovery grant and launch supplements for my IVU Lab on "Learning pixel-level video understanding", postdoc and PhD students interested to apply reach out on my email.
	      <li> March 2024: I am glad to announce that I am an <a href="https://www.cs.ubc.ca/people/mennatullah-siam">affiliate assistant professor</a> with University of British Columbia, Canada. 
	      <li> March 2024: I am a supporting organizer in the first African Computer Vision Summer School, <a href="https://www.acvss.ai/">ACVSS</a>, Nairobi, Kenya co-located in Microsoft Research (MARI).
	      <li> February 2024: 1 Paper got accepted in CVPR 2024 on prompting pixel-level image understanding models, and our work on studying video understanding models from a neuroscience perspective is released on arxiv. 
	      <li> December 2023: I am co-organizing 3rd workshop on L3D-IVU in CVPR 2024.
	      <li> I am an outstanding reviewer in <a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php"> ICCV 2023</a>.
	      <li> July 2023: I started as an assistant professor in Ontario Tech University
	      <li> June 2023: I am a WACV 2024 Area Chair.
	      <li> February 2023: Our paper on Multiscale Video Transformers for Video Object Segmentation is accepted in CVPR 2023.
	      <li> December 2022: Co-organizing <a href="https://sites.google.com/view/l3d-ivu-2023">2nd Workshop on L3D-IVU</a>: Learning with Limited Labelled Data for Image and Video Understanding in CVPR 2023.</li>
	      <li> November 2022: I was a Keynote speaker in Black in AI workshop co-located with Neurips 2022 on <a href="https://slideslive.com/38995267/learning-scene-and-video-understanding-with-limited-labelled-data?ref=search-presentations-Mennatullah+siam">Learning Scene and Video Understanding with Limited Labelled Data</a>.
              <li> September 2022: I am guest editor in the <a href="https://www.mdpi.com/journal/remotesensing/special_issues/75B73YS791">special issue</a> on "Signal Processing and Machine Learning for Autonomous Driving" in Remote Sensing Journal.</li>
              <li> April 2022: Gave a talk on few-shot learning and its extension beyond single images to videos in Samsung AI.</li>
              <li> March 2022: Our <a href="https://arxiv.org/abs/2206.02846">paper</a> on the interpretability of Spatiotemporal models has been accepted in CVPR2022.</li>
              <li> December 2021: Co-organizing <a href="https://sites.google.com/view/l3d-ivu/">Workshop on L3D-IVU</a>: Learning with Limited Labelled Data for Image and Video Understanding in CVPR 2022.</li>
              <li> December 2021: Our <a href="https://ml4ad.github.io/files/papers2021/Temporal%20Transductive%20Inference%20for%20Few-Shot%20Video%20Object%20Segmentation.pdf"> short paper</a> in Machine Learning for Autonomous Driving Workshop in Neurips 2021 was accepted. </li>
              <li> July 2021: Officially Started my Postdoc in York University under supervision from Prof. Richard Wildes and Kostas Derpanis,
              <li> May 2021: I officially finished my PhD and graduated from University of Alberta convocation in Fall 2021, <a href="https://era.library.ualberta.ca/items/2bf2ba86-6201-4e76-b8b3-4b90b1fa3718"> Thesis</a>.
            </ul>
        </div>

     <section id="positions">
      <div class="user-details">
        <h1>IVU Lab</h1>
      </div>
      <h3>Open Positions</h3>
	Thanks for your interest to join my lab. Due to the current disputes with OTU I am no longer accepting students there. <br>
	<ul>
	<li> I am accepting MSc/PhD students through UBC jointly supervised with Prof. Leonid Sigal. Please apply through the university directly following their admission deadlines in Dec. 15.
	<li> I am accepting two PhD interns to join my lab in Summer/Fall 2025, reach out with your resume and transcripts to my email.
	<li> I am accepting one postdoc to join my lab for one year with possible extension to a second year at UBC Vancouver, reach out with your resume to my email. <a href="https://www.postdocs.ubc.ca/ad/58478">More details</a>
	</ul>
	     
      <h3>Graduate Students and interns</h3>
	<ul>
	<li> Yousef Hesham (MSc student, Nile University)
	<li> Abduljaleel Adejumo (MSc student, AMMI/AIMS)
	<li> Mohamed Rashad (MSc intern)
	<li> Omid Reza Heidari (Research Engineer)
	</ul>
	     
      <h3>Postdocs and Researchers</h3>
	<ul>
	<li> Faegheh Yeganli (Research Scientist)
	<li> Maria Siddiqua (Postdoctoral Fellow, awaiting work permit)
	</ul>
	     
      <h3>Alumni</h3>
	<ul>
	<li> Leila Cheshmi (MEng student, Ontario Tech University - 2024)
	<li> Mai Gamal (Visiting PhD Student, GUC - Summer'2023, 2024)
	</ul>
	     
    <!-- Publications Section -->
    <section id="projects">
      <div class="user-details">
        <h1>Publications</h1>
      </div>

      <!-- User Project #1: Personal Résumé Website -->
    <h2>2025</h2>
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="https://raw.githubusercontent.com/MSiam/PixFoundation/0d58b659ffd53b2b03ac5057df465e89656f71b8/imgs/ICML25PixFoundation.drawio.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?</h3>
          <p><u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify">Arxiv. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2502.04192">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/PixFoundation/">Code</a>
        </div>
      </div>
	    
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/ThePowerofOne.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>The Power of One: A Single Example is All it Takes for Segmentation in VLMs</h3>
          <p>Mir Rayat Imtiaz Hossain, <u><strong>Mennatullah Siam</strong></u>, Leonid Sigal, James J. Little</p>
          <p style="text-align: justify">Arxiv. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2503.10779">Paper</a>
        </div>
      </div>
	    
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/IJCV_TTI_Overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Temporal Transductive Inference for Fewshot Video Object Segmentation</h3>
          <p><u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify">IJCV 2025. </p> <br>
          <a class="project-link" target="_blank" href="https://link.springer.com/article/10.1007/s11263-025-02390-x">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/tti_fsvos.git">Code</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=UZxi8zqYAaY&t=94s">Demo</a>
        </div>
      </div>
	    
	</div>
        <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/tamvt.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>TAM-VT: Transformation-Aware Multi-scale Video Transformer for Segmentation and Tracking</h3>
          <p>Raghav Goyal, Wan-Cyuan Fan, <u><strong>Mennatullah Siam</strong></u>, Leonid Sigal</p>
          <p style="text-align: justify"> WACV 2025. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2312.08514">Paper</a>
	  <a class="project-link" target="_blank" href="https://davidhalladay.github.io/m3t_demo/">Project Webpage</a>
	  <a class="project-link" target="_blank" href="https://github.com/davidhalladay/TAM-VT">Code</a>
        </div>
      </div>

     <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/rsmmvp.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>A Vision Centric Remote Sensing Benchmark.</h3>
          <p>Abduljaleel Adejumo*, Faegheh Yeganli*, Clifford Broni-bediako, Aoran Xiao, Naoto Yokoya+, <u><strong>Mennatullah Siam+</strong></u></p>
	  <p>(* equally contributing, + equally advising)</p>
          <p style="text-align: justify">Arxiv. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2503.15816">Paper</a>
          <a class="project-link" target="_blank" href="https://huggingface.co/datasets/IVUlab/RSMMVP">Dataset</a>
        </div>
      </div>
	    
    <h2>2024</h2>
    <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/medvt++.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>MEDVT++: A Unified Multiscale Encoder-Decoder Transformer for Video Segmentation</h3>
          <p>Rezaul Karim, He Zhao, Richard P. Wildes, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> Journal Extension Under Review. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2304.05930.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://rkyuca.github.io/medvt/">Project Webpage</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/dynast_journal.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks</h3>
          <p>Matthew Kowal, <u><strong>Mennatullah Siam</strong></u>, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis</p>
          <p style="text-align: justify"> TPAMI. </p> <br>
          <a class="project-link" target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10682100">Paper</a>
        </div>
      </div>
	    
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/visual_prompting.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach</h3>
          <p>Mir Rayat Imtiaz Hossain, <u><strong>Mennatullah Siam</strong></u>, Leonid Sigal, James J. Little</p>
          <p style="text-align: justify"> CVPR 2024. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2404.11732">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/rayat137/VisualPromptGFSS">Code</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/grsl.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Generalized Few-Shot Semantic Segmentation in Remote Sensing: Challenge and Benchmark</h3>
          <p>Clifford Broni-Bediako, Junshi Xia, Jian Song, Hongruixuan Chen, <u><strong>Mennatullah Siam</strong></u>, Naoto Yokoya</p>
          <p style="text-align: justify"> IEEE Geoscience and Remote Sensing Letters (accepted). </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2409.11227">Paper</a>
        </div>

	<div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src=""/>
          </picture>
        </div>
        <div class="contents">
          <h3>A Survey on African Computer Vision Datasets, Topics and Researchers</h3>
          <p>Abdul-Hakeem Omotayo*, Ashery Mbilinyi*, Lukman Ismaila*, Houcemeddine Turki, Mahmoud Abdien, Karim Gamal, Idriss Tondji, Yvan Pimi, Naome A. Etori, Marwa M. Matar, Clifford Broni-Bediako, Abigail Oppong, Mai Gamal, Eman Ehab, Gbetondji Dovonon, Zainab Akinjobi, Daniel Ajisafe, Oluwabukola G. Adegboro, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> JAIR - Fariness and Bias in AI Special Issue. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2401.11617.pdf">Paper</a>
	  <a class="project-link" target="_blank" href="https://github.com/Ro-ya-cv4Africa/acvdatasets">Datasets List</a>
	  <a class="project-link" target="_blank" href="https://github.com/Ro-ya-cv4Africa/acvsurvey">Code</a>
        </div>
      </div>
	      
        <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/neuroscience.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics</h3>
          <p>Mai Gamal, Mohamed Rashad, Eman Ehab, Saif ElDawlatly, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> Short Paper in NeuroAI Workshop Neurips 2024. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2402.12519.pdf">Paper</a>
        </div>
	    
        <h2>2023</h2>
    <div class="user-projects">
        <div class="images-right">
          <picture>
            <iframe width="360" height="202" src="https://www.youtube.com/embed/_7wEnwduOb4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </picture>
        </div>
        <div class="contents">
          <h3>MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation</h3>
          <p>Rezaul Karim, He Zhao, Richard P. Wildes, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> CVPR 2023. </p> <br>
          <a class="project-link" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_MED-VT_Multiscale_Encoder-Decoder_Video_Transformer_With_Application_To_Object_Segmentation_CVPR_2023_paper.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://rkyuca.github.io/medvt/">Project Webpage</a>
          <a class="project-link" target="_blank" href="https://github.com/rkyuca/medvt">Code</a>
        </div>
      </div>

       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/MMC_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Rezaul Karim, He Zhao, Richard P. Wildes</p>
          <p style="text-align: justify"> Arxiv. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2307.07812.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/MMC-MultiscaleMemory">Code</a>
        </div>
      </div>

       <div class="user-projects">
        <div class="contents">
          <h3>Towards a Better Understanding of the Computer Vision Research Community in Africa</h3>
          <p>Abdul-Hakeem Omotayo, Mai Gamal, Eman Ehab, Gbetondji Dovonon, Zainab Akinjobi, Ismaila Lukman, Houcemeddine Turki, Mahmod Abdien, Idriss Tondji, Abigail Oppong, Yvan Pimi, Karim Gamal, and <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> EAAMO 2023. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2305.06773.pdf">Paper</a>
        </div>
      </div>
	      
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/transductive_inductive.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Two-Stage Joint Transductive and Inductive Learning for Nuclei Segmentation</h3>
          <p>Hesham Ali, Idriss Tondji, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> ML4H Symposium 2023, Findings Track. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2311.08774.pdf">Paper</a>
        </div>
      </div>

      <h2>2022</h2>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/DynaSt_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>A Deeper Dive into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information</h3>
          <p>Matthew Kowal, <u><strong>Mennatullah Siam</strong></u>, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis</p>
          <p style="text-align: justify"> CVPR 2022. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2206.02846">Paper</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=3EWinAPTBkE&t=1s">Video Demo</a>
          <a class="project-link" target="_blank" href="https://yorkucvil.github.io/Static-Dynamic-Interpretability/">Project Webpage</a>
          <a class="project-link" target="_blank" href="https://github.com/YorkUCVIL/Static-Dynamic-Interpretability/">Code Interpretability</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/MATNet_FusionCrossConStudy">Code AVOS</a>
        </div>
      </div>       

      <h2>2021</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/TTI_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Temporal Transductive Inference for Fewshot Video Object Segmentation</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Konstantinos G. Derpanis, Richard P. Wildes</p>
          <p style="text-align: justify"> ML4AD Workshop, Neurips 2021. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2203.14308">Full Paper</a>
          <a class="project-link" target="_blank" href="https://ml4ad.github.io/files/papers2021/Temporal%20Transductive%20Inference%20for%20Few-Shot%20Video%20Object%20Segmentation.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=UZxi8zqYAaY&t=94s">Video Demo</a>
        </div>
      </div>
 
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/vca.jpg" />
          </picture>
        </div>
        <div class="contents">
          <h3>Video Class Agnostic Segmentation Benchmark for Autonomous Driving</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Alex Kendal, Martin Jagersand</p>
          <p style="text-align: justify"> CVPR 2021 Workshops. </p>
          <a class="project-link" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Siam_Video_Class_Agnostic_Segmentation_Benchmark_for_Autonomous_Driving_CVPRW_2021_paper.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://msiam.github.io/vca/">Project Webpage</a>
        </div>
      </div>

      <h2>2020</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/coatt.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Weakly Supervised Few-shot Object Segmentation using Co-attention with Visual and Semantic Embeddings</h3>
          <p><u><strong>Mennatullah Siam*</strong></u>, Naren Doraiswamy*, Boris N. Oreshkin*, Hengshuai Yao, Martin Jagersand (equally contributing)</p>
          <p style="text-align: justify"> IJCAI 2020. </p>
          <a class="project-link" target="_blank" href="https://www.ijcai.org/proceedings/2020/0120.pdf">Paper</a>
        </div>
      </div>

      <h2>2019</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/amp.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>AMP: Adaptive Masked Proxies for Few-Shot Segmentation</h3>
          <a><u><strong>Mennatullah Siam</strong></u>, Boris N. Oreshkin, Martin Jagersand</a>
          <p style="text-align: justify"> ICCV 2019. </p>
          <a class="project-link" target="_blank" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/AdaptiveMaskedProxies">Code</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/ivos.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Video Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting</h3>
          <a> <u><strong>Mennatullah Siam</strong></u>, Chen Jiang, Steve Lu, Laura Petrich, Mosta Gamal, Mohamed Elhoseiny, Martin Jagersand</a>
          <p style="text-align: justify"> ICRA 2019. </p>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/1810.07733">Paper</a>
          <a class="project-link" target="_blank" href="https://msiam.github.io/ivos/">Dataset</a>
        </div>
      </div>
      
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <iframe width="360" height="202" src="https://www.youtube.com/embed/aLcw73dt_Oo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </picture>
        </div>
        <div class="contents">
          <h3>Online Object and Task Learning via Human Roboti Interaction</h3>
          <p> Masood Dehghan*, Zichen Zhang*, <u><strong>Mennatullah Siam*</strong></u>, Jun Jin, Laura Petrich, Martin Jagersand (equally contributing)</p>
          <p style="text-align: justify"> ICRA 2019. </p>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/1809.08722.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=aLcw73dt_Oo">Video Demo</a>
        </div>
      </div>

      <h2>2018</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/rtmotseg.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Real-time Segmentation with Appearance, Motion and Geometry</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Sara Eikerdawy, Mostafa Gamal, Moemen Abdel-Razek, Martin Jagersand, Hong Zhang </p>
          <p style="text-align: justify"> IROS 2018. </p>
          <a class="project-link" target="_blank" href="https://ieeexplore.ieee.org/abstract/document/8594088">Paper</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/modnet.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Moving Object Detection Network for Autonomous Driving</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani, Martin Jagersand, Ahmed El-Sallab</p>
          <p style="text-align: justify"> ITSC 2018. </p>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/1709.04821">Paper</a>
          <a class="project-link" target="_blank" href="http://webdocs.cs.ualberta.ca/~vis/kittimoseg/">Dataset</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=hwP_oQeULfc">Video Demo</a>
          <a class="project-link" target="_blank" href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=DE&NR=102018114229&KC=&FT=E&locale=en_EP\#">Patent</a>
        </div>
      </div>
   
    <!-- Teaching section -->
    <section id="positions">
      <div class="user-details">
        <h1>Teaching</h1>
      </div>
      <h3>Ontario Tech University</h3>
        <ul>
	  <li> Fall 2023, Fall 2024 ELEE2110 Discrete Mathematics, Undergraduate Course. <a href="https://drive.google.com/file/d/12QCMvLONcnYCakq_2mO9tgK0TITr6kzH/view?usp=sharing">Feedback</a>
          <li> Winter 2024, SOFE4620 Machine Learning and Data Mining, Undergraduate Course. <a href="https://drive.google.com/file/d/1OEWB7thLDLaZDWXCtD8b5zhocU7wu75L/view?usp=sharing">Feedback</a>
          <li> Winter 2024, SOFE2715 Data Structures, Undergraduate Course.
	</ul>

      <h3>Nile University</h3>
        <ul>
	  <li> Spring 2023, CIT-670 Computer Vision, Graduate Course.
	  <li> Spring 2022, CIT-670 Computer Vision, Graduate Course. <a href="https://drive.google.com/file/d/1Pp_-SW1s2K39tNsWTic5tES-VEg-9KeU/view?usp=sharing">Feedback</a>
	</ul>

	<h3>University of Alberta</h3>
        <ul>
	  <li> Winter 2021, MM-805 Computer Vision and 3DTV, Graduate Course. <a href="https://drive.google.com/file/d/16X557WCt6zx_wS9HycJJ3ggNCw2tk2jK/view?usp=sharing">Feedback</a>
	</ul>

       <h3>Teaching Samples</h3>
	<ul>
	  <li> Lecture Sample I used in Ontario Tech University Interview on Optimization. <a href="https://drive.google.com/file/d/1SZ4FMTO7UwLpMBbDci43VQYfxQV0JauD/view?usp=sharing">Lecture</a>
	  <li> Assignment Sample I used in MM805 University of Alberta course. <a href="https://drive.google.com/file/d/19tJPauyAJiKjWAzOmNPjpoceY7g2_LX_/view?usp=sharing">Assignment</a>
	  <li> Course Outline of the CIT-690 Computer Vision Course I taught in Nile University. <a href="https://drive.google.com/file/d/1gyDGTu59wVjTVNMXnkc05wtcMQmnJC96/view?usp=sharing">Outline</a>
	</ul>
       <h3>Volunteering Teaching Load</h3>
        <ul>
	  <li> <a href="https://drive.google.com/drive/folders/1IOSwdurSqLjeOQqJxGyiM-UJxHyQU5ML?usp=sharing">ACVSS 2024 Tutorials</a> 
	</ul>
   
<!--    <footer class="footer">
      <p>Template from &copy; Aditya Vikram Singh</p>
      </footer>--!>

    <!-- Import JS: Particles Theme -->
    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <!-- Import JS: Sweet Scroll -->
    <script src="./assets/js/sweet-scroll.min.js"></script>
    <!-- Import JS: Google Analytics -->
    <script src="./assets/js/google-analytics.js"></script>
    <!-- Import JS: Main Script -->
    <script src="./assets/js/main.js"></script>
  </body>
</html>
