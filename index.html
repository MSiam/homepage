<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Metadata of the Webpage -->
    <!-- Character-set Metadata -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <!-- Viewport Metadata -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Description Metadata-->
    <meta name="description" content="Academic Website" />
    <!-- Author Metadata -->
    <meta name="author" content="Mennatullah Siam" />
    <!-- Keyword Metadata -->
    <meta
      name="keywords"
      content="Mennatullah Siam"
    />
    <!-- Webpage Logo -->
    <link rel="shortcut icon" href="./assets/img/favicon.ico" />
    <link rel="stylesheet" href="./assets/css/academicons.min.css"/>

    <!-- Webpage Title -->
    <title>Mennatullah Siam</title>

    <!-- Import CSS: Main Stylesheet -->
    <link rel="stylesheet" href="./assets/css/main.css" />
  </head>

  <body>
    
    <!-- About Section -->
    <section id="about">
      <!-- User Introduction-->
      <div class="user-details">
        <h2 style="text-align:left;">Mennatullah Siam, PhD, PEng</h2>

        <div class="images-right">
        <picture style="text-align:right;">
          <img
            src="./assets/img/jpg/photo.jpg"
            alt="Professional Me"
            width="40%"
            style="border-radius: 50%"
          />
        </picture>
        </div>

	<script>
	window.onload = function () {
	    var dots = document.getElementById("dots");
	    var moreText = document.getElementById("more");
	    var btnText = document.getElementById("myBtn");
            dots.style.display = "inline";
	    btnText.innerHTML = "Read more"; 
	    moreText.style.display = "none";
	};
	</script>
	      
        <div class="contents">
            <h2></h2>
            <h3 style="text-align:left;">Currently, in transition between positions</h3>

            <p style="text-align:left;"><a href="https://calendar.google.com/calendar/u/0?cid=bWVubmEuc2V5YW1AZ21haWwuY29t">Calendar</a>, <a href="https://scholar.google.com/citations?user=AVPds3kAAAAJ&hl=en&oi=ao">Scholar</a>, <a href="https://www.linkedin.com/in/mennatullah-siam-6546508a/">Linkedin</a>, <a href="https://github.com/MSiam">Github</a>, Email: menna.seyam@gmail.com, <a href="https://msiam.github.io/homepage/docs/CV.pdf">Academic CV</a></p>
	    <br>
	    <p style="text-align:left;"> <a href="#projects">Research Highlights</a>, <a href="#teaching">Teaching</a>, <a href="#PhD">My PhD Thesis</a>
        </div>    
        <p align="justify">
	I was an assistant professor in Ontario Tech University since 2023 leading the Image and Video Understanding (IVU) lab and an affiliate professor in UBC. My research interests include pixel-level scene and video understanding, data efficient learning, interpretability and responsible AI. Previously I was a Postdoctoral researcher working with Professor Richard Wildes in York University, 2021-2023. I was also a vector affiliate in 2022. I obtained my PhD in 2021 under Professor Martin Jagersand supervision working in vision for robotics. My thesis was focused on learning video object segmentation from limited labelled data, where I was working on the intersection between video object segmentation and fewshot object segmentation with application to both autonomous driving and robot manipulation. I was a member in a team of 4 in the KUKA Innovation Challenge 2018, where our team received a finalist award. Previously I finished my MSc in NU and BSc in Ainshams University, Egypt.
<br>
<b>Research Interests:</b> Computer Vision, Deep Learning, Robotics, Fewshot Learning, Foundation Models, Video Understanding, Interpretability, Responsible AI.
<br>
<b>Research Statement:</b> <a href="https://msiam.github.io/homepage/docs/research_statement.pdf">This statement</a> only provides a rough description of my research program without delving into details.
<br>
I am heavily promoting "No to Killing children and civilians", "No to Genocide!", "No to undermining Human Rights for some money that will be gone anyway within a limited lifetime". This is an integral component of my teaching and research just to regain humanity back again in a fast paced AI research.
<br>
I am also supportive for African researchers passionate to learn about the Computer Vision field, through founding <a href="https://ro-ya-cv4africa.github.io/homepage/index.html">Ro'ya community</a>, <a href="https://x.com/RoyaCV4Africa">Twitter</a>.
<br>
Note I dont sleep with anyone! So work offers or affiliations that are related to that in the future don't give it to me to begin with cause it won't happen! My apologies for being explicit but we entered quite the dark days that abnormal things are being normalized.
	</p>
      </div>
	    
    <section id="talks">
        <div>
            <h1> Talks and Recognitions</h1>
            <ul>
	      <li> "Learning Image and Video Understanding with Limited Labelled Data", <a href="https://slideslive.com/38995267/learning-scene-and-video-understanding-with-limited-labelled-data">Black in AI Keynote</a>, NeurIPS 2022.
	      <li> "From Image to Video Understanding, what to Consider?", University of British Columbia, 2022.
	      <li> "On the Intersection of Few-shot and Video Object Segmentation.", Doctoral Consortium, CVPR 2021.
	      <li> "Few-shot Learning Tutorial", Samsung AI, 2022.
	      <li> "Segmentation with Transformers Tutorial", <a href="https://msiam.github.io/homepage/docs/TutorialPanopticSeg.pdf">Ro'ya Workshop</a>, Deep Learning Indaba, Accra, Ghana, 2024.
	      <li> <a href="https://www.youtube.com/watch?v=aLcw73dt_Oo">KUKA Innovation Finalist Award</a>, 2018.
	      <li> <a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php">Outstanding Reviewing</a> in ICCV 2023.
	      <li> PhD/Postdoc scholarships: VISTA Fellowship, AITF, Verna Tate Graduate Scholarship, Alberta Graduate Excellence Scholarship.
	   </ul>
	</div>
	    
    <section id="news">
        <div>
            <h1> News</h1>
            <ul>
		  <li> Jan 2026: Our work with UBC on "Segmentation From Attention: Training-Free Layer Selection and One-Shot Tuning for Segmentation in VLMs" has been accepted to TMLR, congratulations to Rayat and also for finishing his PhD recently.
		  <li> Dec 2025: Our work on "Pixel-level Understanding of a World in Motion within a Neural Encoding Framework" has been accepted to Nature Scientific Reports, Motion Perception special issue, congratulations to Mai and the team and for her PhD graduation.
	      <li> June 2025: I am WACV 2026 Area Chair.
	      <li> April 2025: Our work on building a vision centric remote sensing benchmark that was led by AMMI/AIMS MSc student jointly supervised by me and Prof. Naoto has been accepted in Eval-FoMo CVPR 25 workshop. Congrats to the team and to him for finishing his MSc.  
	      <li> February 2025: My work got accepted in IJCV 2025 that is focused on few-shot video object segmentation, I acknowledge the guidance of my postdoc supervisors although they were not able to continue the work till its final acceptance form.
	      <li> January 2025: I am an organizer in <a href="https://sites.google.com/view/pixfoundation"> PixFoundation: 1st Workshop on Pixel-level Vision Foundation Models in CVPR 2025</a>, You can Follow us on <a href="https://x.com/PixFoundationCV"> Twitter </a> for updates.
	      <li> November 2024: Our work on TAM-VT video segmentation and tracking is accepted in WACV 2025, our work with RIKEN institute was also accepted in IEEE Geoscience and Remote Sensing Letters.
	      <li> October 2024: Our work is accepted in Neuro AI workshop and WiML part of NeurIPS 2024.
	      <li> September 2024: Our work is accepted in TPAMI, which was an extension of our CVPR 2022 paper.</li>
	      <li> August 2024: Our work on the current state of Computer Vision research in Africa is accepted in JAIR special issue on Fairness and Bias in AI.
	      <li> June 2024: I am WACV 2025 Area Chair.
	      <li> May 2024: I acquired the NSERC Alliance International grant, thanks to NSERC.
	      <li> April 2024: Happy to announce that I acquired the Discovery grant and launch supplements for my IVU Lab on "Learning pixel-level video understanding", postdoc and PhD students interested to apply reach out on my email.
	      <li> March 2024: I am glad to announce that I am an <a href="https://www.cs.ubc.ca/people/mennatullah-siam">affiliate assistant professor</a> with University of British Columbia, Canada. 
	      <li> March 2024: I am a supporting organizer in the first African Computer Vision Summer School, <a href="https://www.acvss.ai/past-editions/2024">ACVSS</a>, Nairobi, Kenya co-located in Microsoft Research (MARI).
	      <li> February 2024: 1 Paper got accepted in CVPR 2024 on prompting pixel-level image understanding models, and our work on studying video understanding models from a neuroscience perspective is released on arxiv.
	    </ul>
	      <span id="dots">...</span><span id="more">
	    <ul>
	      <li> December 2023: I am co-organizing 3rd workshop on L3D-IVU in CVPR 2024.
	      <li> I am an outstanding reviewer in <a href="https://iccv2023.thecvf.com/outstanding.reviewers-118.php"> ICCV 2023</a>.
	      <li> July 2023: I started as an assistant professor in Ontario Tech University
	      <li> June 2023: I am a WACV 2024 Area Chair.
	      <li> February 2023: Our paper on Multiscale Video Transformers for Video Object Segmentation is accepted in CVPR 2023.
	      <li> December 2022: Co-organizing <a href="https://sites.google.com/view/l3d-ivu-2023">2nd Workshop on L3D-IVU</a>: Learning with Limited Labelled Data for Image and Video Understanding in CVPR 2023.</li>
	      <li> November 2022: I was a Keynote speaker in Black in AI workshop co-located with Neurips 2022 on <a href="https://slideslive.com/38995267/learning-scene-and-video-understanding-with-limited-labelled-data?ref=search-presentations-Mennatullah+siam">Learning Scene and Video Understanding with Limited Labelled Data</a>.
              <li> September 2022: I am guest editor in the <a href="https://www.mdpi.com/journal/remotesensing/special_issues/75B73YS791">special issue</a> on "Signal Processing and Machine Learning for Autonomous Driving" in Remote Sensing Journal.</li>
              <li> April 2022: Gave a talk on few-shot learning and its extension beyond single images to videos in Samsung AI.</li>
              <li> March 2022: Our <a href="https://arxiv.org/abs/2206.02846">paper</a> on the interpretability of Spatiotemporal models has been accepted in CVPR2022.</li>
              <li> December 2021: Co-organizing <a href="https://sites.google.com/view/l3d-ivu/">Workshop on L3D-IVU</a>: Learning with Limited Labelled Data for Image and Video Understanding in CVPR 2022.</li>
              <li> December 2021: Our <a href="https://ml4ad.github.io/files/papers2021/Temporal%20Transductive%20Inference%20for%20Few-Shot%20Video%20Object%20Segmentation.pdf"> short paper</a> in Machine Learning for Autonomous Driving Workshop in Neurips 2021 was accepted. </li>
              <li> July 2021: Officially Started my Postdoc in York University under supervision from Prof. Richard Wildes and Kostas Derpanis,
              <li> May 2021: I officially finished my PhD and graduated from University of Alberta convocation in Fall 2021, <a href="https://era.library.ualberta.ca/items/2bf2ba86-6201-4e76-b8b3-4b90b1fa3718"> Thesis</a>.
            </ul>
	</span><br>
	<button onclick="myFunction()" id="myBtn">Read more</button>
	 <script>
	function myFunction() {
	  var dots = document.getElementById("dots");
	  var moreText = document.getElementById("more");
	  var btnText = document.getElementById("myBtn");
	
	  if (dots.style.display === "none") {
	    dots.style.display = "inline";
	    btnText.innerHTML = "Read more"; 
	    moreText.style.display = "none";
	  } else {
	    dots.style.display = "none";
	    btnText.innerHTML = "Read less"; 
	    moreText.style.display = "inline";
	  }
	}
	</script>
        </div>
	    
     <section id="positions">
      <div class="user-details">
        <h1>IVU Lab - Snapshot May 2025</h1>
      </div>
      <h3>Open Positions</h3>
	Thanks for your interest to join my lab. I am currently not accepting students as I am relocating will post more details soon. <br>
	     
      <h3>Graduate Students and interns</h3>
	<ul>
	<li> Yousef Hesham (MSc student, Nile University)
	<li> Abduljaleel Adejumo (MSc student, AMMI/AIMS)
	<li> Mohamed Rashad (MSc intern)
	<li> Omid Reza Heidari (Research Engineer)
	</ul>
	     
      <h3>Postdocs and Researchers</h3>
	<ul>
	<li> Faegheh Yeganli (Research Scientist)
	<li> Maria Siddiqua (Postdoctoral Fellow, awaiting work permit)
	</ul>
	     
      <h3>Alumni</h3>
	<ul>
	<li> Leila Cheshmi (MEng student, Ontario Tech University - 2024)
	<li> Mai Gamal (Visiting PhD Student, GUC - Summer'2023, 2024)
	</ul>
	     
    <!-- Publications Section -->
    <section id="projects">
      <div class="user-details">
        <h1>Publications</h1>
      </div>

      <!-- User Project #1: Personal Résumé Website -->
     <h2>2026</h2>
	      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/ThePowerofOne.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>The power of one: A single example is all it takes for segmentation in vlms.</h3>
          <p>Mir Rayat Hossain, <u><strong>Mennatullah Siam</strong></u>, Leonid Sigal, Jim Little</p>
          <p style="text-align: justify">TMLR. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2503.10779">Paper</a>
        </div>

		 <div class="images-right">
          <picture>
            <img alt="" src="" />
          </picture>
        </div>
        <div class="contents">
          <h3>Pixel-level Understanding of a World in Motion within a Neural Encoding Framework.</h3>
          <p>Mai Gamal, Mohamed Rashad, Eman Ehab, Saif ElDawlatly, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify">Nature Scientific Reports (Motion Perception Collection). </p> <br>
          <a class="project-link" target="_blank" href="">Paper (in-press)</a>
        </div>
      </div>
		
    <h2>2025</h2>
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="assets/img/jpg/Neurips25_mocentric_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</h3>
          <p><u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify">Arxiv. </p> <br>
		  <a class="project-link" target="_blank" href="https://msiam.github.io/PixFoundationSeries/">Project Webpage</a>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2509.02807">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/PixFoundation-2.0/">Code</a>
        </div>
      </div>
	
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="https://raw.githubusercontent.com/MSiam/PixFoundation/0d58b659ffd53b2b03ac5057df465e89656f71b8/imgs/ICML25PixFoundation.drawio.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?</h3>
          <p><u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify">Arxiv. </p> <br>
		  <a class="project-link" target="_blank" href="https://msiam.github.io/PixFoundationSeries/">Project Webpage</a>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2502.04192">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/PixFoundation/">Code</a>
		  <a class="project-link" target="_blank" href="https://huggingface.co/IVUlab">Datasets</a>
        </div>
      </div>
	    
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/IJCV_TTI_Overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Temporal Transductive Inference for Fewshot Video Object Segmentation</h3>
          <p><u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify">IJCV 2025. </p> <br>
          <a class="project-link" target="_blank" href="https://link.springer.com/article/10.1007/s11263-025-02390-x">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/tti_fsvos.git">Code</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=UZxi8zqYAaY&t=94s">Demo</a>
        </div>
      </div>
	    
	</div>
        <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/tamvt.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>TAM-VT: Transformation-Aware Multi-scale Video Transformer for Segmentation and Tracking</h3>
          <p>Raghav Goyal, Wan-Cyuan Fan, <u><strong>Mennatullah Siam</strong></u>, Leonid Sigal</p>
          <p style="text-align: justify"> WACV 2025. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2312.08514">Paper</a>
	  <a class="project-link" target="_blank" href="https://davidhalladay.github.io/m3t_demo/">Project Webpage</a>
	  <a class="project-link" target="_blank" href="https://github.com/davidhalladay/TAM-VT">Code</a>
        </div>
      </div>
	
     <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/rsmmvp.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>A Vision Centric Remote Sensing Benchmark.</h3>
          <p>Abduljaleel Adejumo*, Faegheh Yeganli*, Clifford Broni-bediako, Aoran Xiao, Naoto Yokoya+, <u><strong>Mennatullah Siam+</strong></u></p>
	  <p>(* equally contributing, + equally advising)</p>
          <p style="text-align: justify">Eval-FoMo Workshop, CVPR 2025, Non Archival. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2503.15816">Paper</a>
          <a class="project-link" target="_blank" href="https://huggingface.co/datasets/IVUlab/RSMMVP">Dataset</a>
        </div>
      </div>

	  <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/msvideotransformer.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving.</h3>
          <p>Leila Cheshmi, <u><strong>Mennatullah Siam</strong></u></p>
	  <p></p>
          <p style="text-align: justify">Arxiv. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2508.14729">Paper</a>
        </div>
      </div>

    <h2>2024</h2>
    <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/medvt++.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>MEDVT++: A Unified Multiscale Encoder-Decoder Transformer for Video Segmentation</h3>
          <p>Rezaul Karim, He Zhao, Richard P. Wildes, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> Journal Extension Under Review. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2304.05930.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://rkyuca.github.io/medvt/">Project Webpage</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/dynast_journal.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks</h3>
          <p>Matthew Kowal, <u><strong>Mennatullah Siam</strong></u>, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis</p>
          <p style="text-align: justify"> TPAMI. </p> <br>
          <a class="project-link" target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10682100">Paper</a>
        </div>
      </div>
	    
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/visual_prompting.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach</h3>
          <p>Mir Rayat Imtiaz Hossain, <u><strong>Mennatullah Siam</strong></u>, Leonid Sigal, James J. Little</p>
          <p style="text-align: justify"> CVPR 2024. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2404.11732">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/rayat137/VisualPromptGFSS">Code</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/grsl.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Generalized Few-Shot Semantic Segmentation in Remote Sensing: Challenge and Benchmark</h3>
          <p>Clifford Broni-Bediako, Junshi Xia, Jian Song, Hongruixuan Chen, <u><strong>Mennatullah Siam</strong></u>, Naoto Yokoya</p>
          <p style="text-align: justify"> IEEE Geoscience and Remote Sensing Letters (accepted). </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2409.11227">Paper</a>
        </div>

	<div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src=""/>
          </picture>
        </div>
        <div class="contents">
          <h3>A Survey on African Computer Vision Datasets, Topics and Researchers</h3>
          <p>Abdul-Hakeem Omotayo*, Ashery Mbilinyi*, Lukman Ismaila*, Houcemeddine Turki, Mahmoud Abdien, Karim Gamal, Idriss Tondji, Yvan Pimi, Naome A. Etori, Marwa M. Matar, Clifford Broni-Bediako, Abigail Oppong, Mai Gamal, Eman Ehab, Gbetondji Dovonon, Zainab Akinjobi, Daniel Ajisafe, Oluwabukola G. Adegboro, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> JAIR - Fariness and Bias in AI Special Issue. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2401.11617.pdf">Paper</a>
	  <a class="project-link" target="_blank" href="https://github.com/Ro-ya-cv4Africa/acvdatasets">Datasets List</a>
	  <a class="project-link" target="_blank" href="https://github.com/Ro-ya-cv4Africa/acvsurvey">Code</a>
        </div>
      </div>
	      
        <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/neuroscience.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics</h3>
          <p>Mai Gamal, Mohamed Rashad, Eman Ehab, Saif ElDawlatly, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> Short Paper in NeuroAI Workshop Neurips 2024 & WiML workshop. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2402.12519.pdf">Paper</a>
        </div>
	    
        <h2>2023</h2>
    <div class="user-projects">
        <div class="images-right">
          <picture>
            <iframe width="360" height="202" src="https://www.youtube.com/embed/_7wEnwduOb4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </picture>
        </div>
        <div class="contents">
          <h3>MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation</h3>
          <p>Rezaul Karim, He Zhao, Richard P. Wildes, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> CVPR 2023. </p> <br>
          <a class="project-link" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_MED-VT_Multiscale_Encoder-Decoder_Video_Transformer_With_Application_To_Object_Segmentation_CVPR_2023_paper.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://rkyuca.github.io/medvt/">Project Webpage</a>
          <a class="project-link" target="_blank" href="https://github.com/rkyuca/medvt">Code</a>
        </div>
      </div>

       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/MMC_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Rezaul Karim, He Zhao, Richard P. Wildes</p>
          <p style="text-align: justify"> Arxiv. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2307.07812.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/MMC-MultiscaleMemory">Code</a>
        </div>
      </div>

       <div class="user-projects">
        <div class="contents">
          <h3>Towards a Better Understanding of the Computer Vision Research Community in Africa</h3>
          <p>Abdul-Hakeem Omotayo, Mai Gamal, Eman Ehab, Gbetondji Dovonon, Zainab Akinjobi, Ismaila Lukman, Houcemeddine Turki, Mahmod Abdien, Idriss Tondji, Abigail Oppong, Yvan Pimi, Karim Gamal, and <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> EAAMO 2023. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2305.06773.pdf">Paper</a>
        </div>
      </div>
	      
       <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/transductive_inductive.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Two-Stage Joint Transductive and Inductive Learning for Nuclei Segmentation</h3>
          <p>Hesham Ali, Idriss Tondji, <u><strong>Mennatullah Siam</strong></u></p>
          <p style="text-align: justify"> ML4H Symposium 2023, Findings Track. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/2311.08774.pdf">Paper</a>
        </div>
      </div>

      <h2>2022</h2>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/DynaSt_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>A Deeper Dive into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information</h3>
          <p>Matthew Kowal, <u><strong>Mennatullah Siam</strong></u>, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis</p>
          <p style="text-align: justify"> CVPR 2022. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2206.02846">Paper</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=3EWinAPTBkE&t=1s">Video Demo</a>
          <a class="project-link" target="_blank" href="https://yorkucvil.github.io/Static-Dynamic-Interpretability/">Project Webpage</a>
          <a class="project-link" target="_blank" href="https://github.com/YorkUCVIL/Static-Dynamic-Interpretability/">Code Interpretability</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/MATNet_FusionCrossConStudy">Code AVOS</a>
        </div>
      </div>       

      <h2>2021</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/TTI_overview.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Temporal Transductive Inference for Fewshot Video Object Segmentation</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Konstantinos G. Derpanis, Richard P. Wildes</p>
          <p style="text-align: justify"> ML4AD Workshop, Neurips 2021. </p> <br>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/2203.14308">Full Paper</a>
          <a class="project-link" target="_blank" href="https://ml4ad.github.io/files/papers2021/Temporal%20Transductive%20Inference%20for%20Few-Shot%20Video%20Object%20Segmentation.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=UZxi8zqYAaY&t=94s">Video Demo</a>
        </div>
      </div>
 
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/vca.jpg" />
          </picture>
        </div>
        <div class="contents">
          <h3>Video Class Agnostic Segmentation Benchmark for Autonomous Driving</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Alex Kendal, Martin Jagersand</p>
          <p style="text-align: justify"> CVPR 2021 Workshops. </p>
          <a class="project-link" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Siam_Video_Class_Agnostic_Segmentation_Benchmark_for_Autonomous_Driving_CVPRW_2021_paper.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://msiam.github.io/vca/">Project Webpage</a>
        </div>
      </div>

      <h2>2020</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/coatt.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Weakly Supervised Few-shot Object Segmentation using Co-attention with Visual and Semantic Embeddings</h3>
          <p><u><strong>Mennatullah Siam*</strong></u>, Naren Doraiswamy*, Boris N. Oreshkin*, Hengshuai Yao, Martin Jagersand (equally contributing)</p>
          <p style="text-align: justify"> IJCAI 2020. </p>
          <a class="project-link" target="_blank" href="https://www.ijcai.org/proceedings/2020/0120.pdf">Paper</a>
        </div>
      </div>

      <h2>2019</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/amp.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>AMP: Adaptive Masked Proxies for Few-Shot Segmentation</h3>
          <a><u><strong>Mennatullah Siam</strong></u>, Boris N. Oreshkin, Martin Jagersand</a>
          <p style="text-align: justify"> ICCV 2019. </p>
          <a class="project-link" target="_blank" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://github.com/MSiam/AdaptiveMaskedProxies">Code</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/ivos.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Video Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting</h3>
          <a> <u><strong>Mennatullah Siam</strong></u>, Chen Jiang, Steve Lu, Laura Petrich, Mosta Gamal, Mohamed Elhoseiny, Martin Jagersand</a>
          <p style="text-align: justify"> ICRA 2019. </p>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/1810.07733">Paper</a>
          <a class="project-link" target="_blank" href="https://msiam.github.io/ivos/">Dataset</a>
        </div>
      </div>
      
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <iframe width="360" height="202" src="https://www.youtube.com/embed/aLcw73dt_Oo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </picture>
        </div>
        <div class="contents">
          <h3>Online Object and Task Learning via Human Roboti Interaction</h3>
          <p> Masood Dehghan*, Zichen Zhang*, <u><strong>Mennatullah Siam*</strong></u>, Jun Jin, Laura Petrich, Martin Jagersand (equally contributing)</p>
          <p style="text-align: justify"> ICRA 2019. </p>
          <a class="project-link" target="_blank" href="https://arxiv.org/pdf/1809.08722.pdf">Paper</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=aLcw73dt_Oo">Video Demo</a>
        </div>
      </div>

      <h2>2018</h2>
      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/rtmotseg.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Real-time Segmentation with Appearance, Motion and Geometry</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Sara Eikerdawy, Mostafa Gamal, Moemen Abdel-Razek, Martin Jagersand, Hong Zhang </p>
          <p style="text-align: justify"> IROS 2018. </p>
          <a class="project-link" target="_blank" href="https://ieeexplore.ieee.org/abstract/document/8594088">Paper</a>
        </div>
      </div>

      <div class="user-projects">
        <div class="images-right">
          <picture>
            <img alt="" src="./assets/img/jpg/modnet.png" />
          </picture>
        </div>
        <div class="contents">
          <h3>Moving Object Detection Network for Autonomous Driving</h3>
          <p><u><strong>Mennatullah Siam</strong></u>, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani, Martin Jagersand, Ahmed El-Sallab</p>
          <p style="text-align: justify"> ITSC 2018. </p>
          <a class="project-link" target="_blank" href="https://arxiv.org/abs/1709.04821">Paper</a>
          <a class="project-link" target="_blank" href="http://webdocs.cs.ualberta.ca/~vis/kittimoseg/">Dataset</a>
          <a class="project-link" target="_blank" href="https://www.youtube.com/watch?v=hwP_oQeULfc">Video Demo</a>
          <a class="project-link" target="_blank" href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=DE&NR=102018114229&KC=&FT=E&locale=en_EP\#">Patent</a>
        </div>
      </div>
		
    <!-- Teaching section -->
    <section id="teaching">
      <div class="user-details">
        <h1>Teaching</h1>
      </div>
      <h3>Ontario Tech University</h3>
        <ul>
	  <li> Fall 2023, Fall 2024 ELEE2110 Discrete Mathematics, Undergraduate Course. <a href="https://msiam.github.io/homepage/docs/2023-Discrete-Math-Course-Outline.pdf"> Course Outline</a>, <a href="https://msiam.github.io/homepage/docs/DM_ELEE2110_OTU_Feedback.pdf">Feedback</a>
          <li> Winter 2024, SOFE4620 Machine Learning and Data Mining, Undergraduate Course. <a href="https://msiam.github.io/homepage/docs/W2024-ML-Course-Outline.pdf"> Course Outline</a>, <a href="https://msiam.github.io/homepage/docs/ML_SOFE4620_OTU_Feedbackpdf.pdf">Feedback</a>
          <li> Winter 2024, SOFE2715 Data Structures, Undergraduate Course. 
	</ul>

      <h3>Nile University</h3>
        <ul>
	  <li> Spring 2023, CIT-670 Computer Vision, Graduate Course.
	  <li> Spring 2022, CIT-670 Computer Vision, Graduate Course. <a href="https://msiam.github.io/homepage/docs/NU_CV.pdf"> Course Outline</a>, <a href="https://msiam.github.io/homepage/docs/CV_CIT690_NU_Feedback.pdf">Feedback</a>
	</ul>

	<h3>University of Alberta</h3>
        <ul>
	  <li> Winter 2021, MM-805 Computer Vision and 3DTV, Graduate Course. <a href="https://msiam.github.io/homepage/docs/CV_MM805_UofA_Feedback.pdf">Feedback</a>
	</ul>

       <h3>Teaching Samples</h3>
	<ul>
	  <li> Lecture Sample I used in Ontario Tech University Interview on Optimization. <a href="https://msiam.github.io/homepage/docs/SampleLecture_UOIT.pdf">Lecture</a>
	  <li> Assignment Sample I used in MM805 University of Alberta course. <a href="https://msiam.github.io/homepage/docs/CV_MM805_UofA_Assignment.pdf">Assignment</a>
	</ul>
       <h3>Volunteer Teaching</h3>
        <ul>
	  <li> <a href="https://www.youtube.com/playlist?list=PL4jKsHbreeuDK-QNIYnB8M1qPoTqdzr1y">ACVSS 2024 Tutorials</a> 
	</ul>

     <section id="PhD">
      <div class="user-details">
        <h1>PhD Thesis</h1>
      </div>
	  <p>Learning Video Object Segmentation from Limited Labelled Data</p>
	  <a href="https://ualberta.scholaris.ca/items/a29ed761-60ad-4d54-9b29-ce07d7787cfd">PDF</a>
	  <a href="https://msiam.github.io/homepage/docs/PhDDegree.pdf">Degree</a>
	  <a href="https://msiam.github.io/homepage/docs/UASAC016_TranscriptsPhD.pdf">Transcripts w/ Awards</a>
	  
    <!-- Import JS: Particles Theme -->
    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <!-- Import JS: Sweet Scroll -->
    <script src="./assets/js/sweet-scroll.min.js"></script>
    <!-- Import JS: Google Analytics -->
    <script src="./assets/js/google-analytics.js"></script>
    <!-- Import JS: Main Script -->
    <script src="./assets/js/main.js"></script>
  </body>
</html>
